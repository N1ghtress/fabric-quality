{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8ff015",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846fd0e",
   "metadata": {},
   "source": [
    "## Analyse des données\n",
    "\n",
    "- On a des fichier CSV et h5\n",
    "    - h5 contenant les images en 32x32 et 64x64\n",
    "    - CSV dataframe contenant des informations sur les images des fichier h5\n",
    "- Ici utilisation des images en 64x64\n",
    "- On a un dataframe contenant les inforamtions sur les images contenu dans le fichier h5\n",
    "- On a 6 types de valeurs :\n",
    "    + good\n",
    "    + color\n",
    "    + cut\n",
    "    + hole\n",
    "    + metal_contamination\n",
    "    + thread\n",
    "- Pour simplifier l'anomalie détection on regroupe les types de problèmes autre que good sous le même label \"Damaged\"\n",
    "- Les données ne sont plus équilibré :\n",
    "    + Au départ on à 8000 de chaque type\n",
    "    + Après le regroupement on a :\n",
    "        * 8000 good\n",
    "        * 40000 damaged\n",
    "- On va donc rééquilibré le set pour l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f730304a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the `train64.csv` file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_df64 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/train64.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_df64[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindication_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load the `train64.csv` file\n",
    "train_df64 = pd.read_csv(\"./data/train64.csv\")\n",
    "train_df64['indication_type'].value_counts().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# change les labels autre au que good par damaged et les valeurs correspondante dans le dataframe\n",
    "train_df64[\"indication_type\"] = train_df64.indication_type.apply(lambda row: \"damaged\" if row!=\"good\" else \"good\")\n",
    "train_df64[\"indication_value\"] = train_df64.indication_value.apply(lambda row: 1 if row!=0 else 0)\n",
    "train_df64['indication_type'].value_counts().plot(kind='bar')\n",
    "print(train_df64['indication_type'].value_counts())\n",
    "print(train_df64.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede16b0",
   "metadata": {},
   "source": [
    "## Récupération des images a partir du fichier h5\n",
    "\n",
    "(code récupéré depuis le notebook kaggle d'où est tiré le [dataset](https://www.kaggle.com/code/aadiadgaonkar/isolationforest-lof-gauss-svm-anomaly-detection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object that will take the dataset and produce the dataset in a format required for tensorflow dataset's API\n",
    "class H5ToStorage:\n",
    "  def __init__(self, hdf_path, ds_name=\"train\"):\n",
    "    self.path = hdf_path\n",
    "\n",
    "    self.classes = []\n",
    "    with h5py.File(self.path, 'r') as hf:\n",
    "      for class_ in hf:\n",
    "        self.classes.append(class_)\n",
    "\n",
    "    self.name = ds_name\n",
    "\n",
    "  # a generator to load the (img, class, angle)\n",
    "  def generate_img_arr(self):\n",
    "    for class_ in self.classes:\n",
    "      with h5py.File(self.path, 'r') as hf:\n",
    "        for angle in hf[class_]:\n",
    "            for img in hf[class_][f\"{angle}\"]:\n",
    "                yield img, class_, angle\n",
    "  \n",
    "  # utilize the generator to create new images and load it back to Storage\n",
    "  def generate_train_dirs(self):\n",
    "    # create the dataset's directories\n",
    "    path = \"data/working/train\"\n",
    "    os.makedirs(f\"{path}/good/\")\n",
    "    os.makedirs(f\"{path}/damaged/\")\n",
    "\n",
    "    # random_bright = tf.keras.layers.RandomBrightness(factor=0.05)\n",
    "    random_flip = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")\n",
    "\n",
    "    gen = self.generate_img_arr()\n",
    "    metadata = {}\n",
    "\n",
    "    for i, data in enumerate(gen):\n",
    "        img, label, angle = data\n",
    "        if label == \"good\":\n",
    "          for j in range(4):\n",
    "            img_path = f\"{path}/{label}/{i}_aug{j}.jpeg\"\n",
    "            img = random_flip(tf.expand_dims(np.squeeze(img), axis=2)*255., training=True)\n",
    "            plt.imsave(img_path, np.squeeze(img), cmap=\"gray\")\n",
    "        else:\n",
    "          img_path = f\"{path}/damaged/{i}.jpeg\"\n",
    "          plt.imsave(img_path, np.squeeze(img)*255., cmap=\"gray\")\n",
    "\n",
    "        metadata[img_path] = angle\n",
    "    return metadata\n",
    "\n",
    "  def generate_test_dirs(self):\n",
    "    # create the dataset's directories\n",
    "    path = \"data/working/test\"\n",
    "    os.makedirs(f\"{path}/good/\")\n",
    "    os.makedirs(f\"{path}/damaged/\")\n",
    "\n",
    "      \n",
    "    gen = self.generate_img_arr()\n",
    "    metadata = {}\n",
    "\n",
    "    for i, data in enumerate(gen):\n",
    "        img, label, angle = data\n",
    "        if label == \"good\":\n",
    "          img_path = f\"{path}/{label}/{i}.jpeg\"\n",
    "          plt.imsave(img_path, np.squeeze(img)*255., cmap=\"gray\")\n",
    "        else:\n",
    "          img_path = f\"{path}/damaged/{i}.jpeg\"\n",
    "          plt.imsave(img_path, np.squeeze(img)*255., cmap=\"gray\")\n",
    "\n",
    "        metadata[img_path] = angle\n",
    "\n",
    "    return metadata\n",
    "\n",
    "  def to_storage(self):\n",
    "    if self.name == \"train\":\n",
    "      self.generate_train_dirs()\n",
    "\n",
    "    elif self.name == \"test\":\n",
    "      self.generate_test_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ee5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data & test data paths\n",
    "test_dir = \"data/working/test\"\n",
    "train_dir = \"data/working/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c19ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour ne pas avoir a recréer les images si on les a deja\n",
    "if not(os.path.isdir(test_dir) and os.path.isdir(train_dir)) :\n",
    "    # generate train data\n",
    "    train_gen = H5ToStorage(\"data/matchingtDATASET_train_64.h5\", \"train\")\n",
    "    train_dict = train_gen.to_storage()\n",
    "    # generate train data\n",
    "    test_gen = H5ToStorage(\"data/matchingtDATASET_test_64.h5\", \"test\")\n",
    "    test_dict = test_gen.to_storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94460d95",
   "metadata": {},
   "source": [
    "### Note transformation image pour la création du dataframe d'apprentissage\n",
    "\n",
    "- Modifications apportés aux images :\n",
    "    + Image transformée en nuance de gris via openCV2\n",
    "    + Image flatten via numpy.flatten()\n",
    "    + Ajout de l'image dans le dataframe pour pouvoir apprendre dessus en suite\n",
    "    \n",
    "- Séparation en 2 dataframes avec : \n",
    "    + Les images labeled good (fichier TrainSetGood.csv)\n",
    "    + Les images labeled damaged (fichier TrainSetDamaged.csv)\n",
    "- Perte de la labelisation lors de la création des dataset puisque les données sont séparer dans 2 fichiers\n",
    "    + Si besoin lors de l'apprentissage rajouter le label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0d520",
   "metadata": {},
   "source": [
    "## Création des fichiers CSV pour l'apprentissage\n",
    "#### [tuto progress bar](https://stackoverflow.com/questions/38861829/how-do-i-implement-a-progress-bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c59ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/working/train/\"\n",
    "file_path = \"./data/TrainSet\"\n",
    "ext = \".csv\"\n",
    "\n",
    "dataTrainGood = np.empty([48000,4096], dtype=int)\n",
    "\n",
    "progressGood = IntProgress(min=0, max=100) # instantiate the bar\n",
    "\n",
    "display(progressGood) # display the bar\n",
    "\n",
    "for i, name in enumerate(os.listdir(path+\"good/\")) :\n",
    "    img = cv2.imread(path+\"good/\"+name,0)\n",
    "    img = img.flatten()\n",
    "    dataTrainGood[i] = img\n",
    "    if i %(48000/100) == 0 :\n",
    "        progressGood.value += 1\n",
    "\n",
    "dataTrainGood = pd.DataFrame(dataTrainGood)\n",
    "dataTrainGood.to_csv(file_path+\"Good\"+ext, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978433de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTrainDamaged = np.empty([60000,4096], dtype=int)\n",
    "\n",
    "progressDamaged = IntProgress(min=0, max=100) # instantiate the bar\n",
    "\n",
    "display(progressDamaged) # display the bar\n",
    "for i, name in enumerate(os.listdir(path+\"damaged/\")) :\n",
    "    img = cv2.imread(path+\"damaged/\"+name,0)\n",
    "    img = img.flatten()\n",
    "    dataTrainDamaged[i] = img\n",
    "    if i %(60000/100) == 0 :\n",
    "        progressDamaged.value += 1\n",
    "\n",
    "dataTrainDamaged = pd.DataFrame(dataTrainDamaged)\n",
    "dataTrainDamaged.to_csv(file_path+\"Damaged\"+ext, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd37791",
   "metadata": {},
   "source": [
    "## Code pour undersample les données damaged\n",
    "\n",
    "## Attention : ici le code utilise les données du fichier CSV original et non celui des fichiers créer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "damagedData = train_df64[train_df64['indication_type'] == 'damaged']\n",
    "goodData = train_df64[train_df64['indication_type'] == 'good']\n",
    "num = min(len(damagedData),len(goodData))\n",
    "print(damagedData['indication_type'].value_counts())\n",
    "print()\n",
    "print(goodData['indication_type'].value_counts())\n",
    "data = pd.concat([damagedData.sample(num,random_state=2),goodData.sample(num, random_state=0)],keys= damagedData.keys())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d542804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "num_dmgd, num_good = train_df64.indication_value.value_counts()\n",
    "num = np.min((num_dmgd, num_good))\n",
    "good = train_df64[train_df64['indication_value'] == 0]\n",
    "dmgd = train_df64[train_df64['indication_value'] == 1]\n",
    "df_good = good.sample(num)\n",
    "df_dmgd = dmgd.sample(num)\n",
    "\n",
    "undersampled_df = pd.concat([df_good,df_dmgd],axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
